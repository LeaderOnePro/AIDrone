#!/usr/bin/env python3
"""
è°ƒè¯•GLMæ¨¡å‹APIå“åº”çš„è„šæœ¬
"""
import os
import json
import requests
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

def debug_glm_api():
    """ç›´æ¥æµ‹è¯•GLM API"""
    api_key = os.environ.get("GLM_API_KEY")
    if not api_key:
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ°GLM_API_KEYç¯å¢ƒå˜é‡")
        return
    
    # GLM APIé…ç½®
    base_url = "https://open.bigmodel.cn/api/paas/v4/chat/completions"
    
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    payload = {
        "model": "glm-4.5",
        "messages": [
            {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"}
        ],
        "max_tokens": 100,
        "temperature": 0.5,
    }
    
    print("ğŸ”„ å‘é€APIè¯·æ±‚...")
    print(f"URL: {base_url}")
    print(f"Payload: {json.dumps(payload, indent=2, ensure_ascii=False)}")
    
    try:
        response = requests.post(base_url, headers=headers, json=payload)
        print(f"âœ… HTTPçŠ¶æ€ç : {response.status_code}")
        
        if response.status_code == 200:
            response_data = response.json()
            print("âœ… APIå“åº”æˆåŠŸ")
            print(f"å®Œæ•´å“åº”: {json.dumps(response_data, indent=2, ensure_ascii=False)}")
            
            if 'choices' in response_data and len(response_data['choices']) > 0:
                message = response_data['choices'][0]['message']
                content = message.get('content', '')
                print(f"ğŸ“ æå–çš„å†…å®¹: '{content}'")
            else:
                print("âš ï¸ å“åº”ä¸­æ²¡æœ‰choicesæˆ–choicesä¸ºç©º")
        else:
            print(f"âŒ APIè¯·æ±‚å¤±è´¥")
            print(f"é”™è¯¯ä¿¡æ¯: {response.text}")
            
    except Exception as e:
        print(f"âŒ è¯·æ±‚å¼‚å¸¸: {str(e)}")

if __name__ == "__main__":
    debug_glm_api()
